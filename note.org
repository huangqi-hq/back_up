* Sparse AUtoencoder

* Variational AutoEncoder
VAE将经过神经网络编码后的隐藏层假设为一个标准的高斯分布，然后再从这个分布中采样一个特征，再用这个特征进行解码，期望得到与原始输入相同的结果，损失和AE几乎一样，只是增加编码推断分布与标准高斯分布的KL散度的正则项。
显然增加这个正则项的目的就是防止模型退化成普通的AE。
因为网络训练时为了尽量减小重构误差，必然使得方差逐渐被降到0，这样便不再会有随机采样噪声，也就变成了普通的AE。
VAE为每个输入x，生成了一个潜在概率分布p(z|x)，然后再从分布中进行随机采样，从而得到了连续完整的潜在空间，解决了AE中无法用于生成的问题。
把原始输入x看作是一个表面特征，而其潜在特征便是表面经过抽象之后的类特征，它将比表面特征更具备区分事物的能力，而VAE直接拟合了基于已知的潜在概率分布，可以说是进一步的掌握了事物的本质。

为了避免计算 KL divergence 中的积分，我们使用重参数的技巧，不是每次产生一个隐含向量，而是生成两个向量，一个表示均值，一个表示标准差，这里我们默认编码之后的隐含向量服从一个正态分布的之后，就可以用一个标准正态分布先乘上标准差再加上均值来合成这个正态分布，最后 loss 就是希望这个生成的正态分布能够符合一个标准正态分布，也就是希望均值为 0，方差为 1 。


* 字体
*粗体*
/斜体/
+删除线+
_下划线_
下标： H_2 O
上标： E=mc^2
等宽字：  =git=  或者 ～git～
| Name  | Pone | Age |
|-------+------+-----|
| Peter | 1234 | 17  |
| Anna  | 4321 | 25  |
