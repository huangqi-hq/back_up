* Sparse AUtoencoder
往往提取好的特征的另外一种约束就是稀疏性，通过在损失函数中添加一个合适的项，使得自编码器努力去减少编码层中活跃的神经元。例如，它可以使得编码层平均只有5%的活跃神经元，这就迫使自编码器去将每个输入表示为少量激活的组合。结果，编码层中的每个神经元通常都会代表一个有用的特征（如果您每个月只能说几个字，您肯定会字字千金）。

为了支持稀疏模型，我们首先必须在每次训练迭代中计算编码层的实际稀疏度。 我们通过计算整个训练batch中，编码层中的每个神经元的平均激活情况来实现。 这里的训练batch不能太小，否则平均数不准确。

我们有了每个神经元的平均激活情况，我们希望通过向损失函数添加稀疏损失来惩罚太活跃的神经元。例如，如果我们计算一个神经元的平均激活值为 0.3，但目标稀疏度为0.1，那么它必须受到惩罚才能降低神经元的活跃度。一种方法可以简单地将平方误差(0.3-0.1)^2添加到损失函数中，但实际上更好的方法是使用Kullback-Leibler散度，其具有比均方误差更强的梯度，如下图所示：

我们计算了编码层中每一个神经元的稀疏损失，我们就可以把它们累加起来添加到损失函数中了。为了控制稀疏损失和重构损失的相对重要性，我们可以用稀疏权重这个超参数乘以稀疏损失。如果这个权重太高，模型会紧贴目标稀疏度，但它可能无法正确重建输入，导致模型无用。相反，如果它太低，模型将大多忽略稀疏目标，进而不会学习任何有趣的功能。

稀疏编码器一般用来学习特征，反映数据集的独特统计特征，而不是简单地充当恒等函数，以这种方式训练，执行附带稀疏惩罚的复制任务可以得到学习到的有用特征的模型。

*损失函数*
稀疏自编码器（又称稀疏自动编码机）中，重构误差中添加了一个稀疏惩罚，用来限定任何时刻的隐藏层中并不是所有单元都被激活。如果 m 是输入模式的总数，那么可以定义一个参数 ρ_hat，用来表示每个隐藏层单元的行为（平均激活多少次）。基本的想法是让约束值 ρ_hat 等于稀疏参数 ρ。具体实现时在原始损失函数中增加表示稀疏性的正则项，损失函数如下：
loss = Mean_squared_error + Regularization_for_sparsity_parameter
( 损失 = 均方误差 + 稀疏参数正则项 )
如果 ρ_hat 偏离 ρ，那么正则项将惩罚网络，一个常规的实现方法是衡量 ρ 和 ρ_hat 之间的 Kullback-Leiber (KL) 散度。

* Variational AutoEncoder
VAE将经过神经网络编码后的隐藏层假设为一个标准的高斯分布，然后再从这个分布中采样一个特征，再用这个特征进行解码，期望得到与原始输入相同的结果，损失和AE几乎一样，只是增加编码推断分布与标准高斯分布的KL散度的正则项。
显然增加这个正则项的目的就是防止模型退化成普通的AE。
因为网络训练时为了尽量减小重构误差，必然使得方差逐渐被降到0，这样便不再会有随机采样噪声，也就变成了普通的AE。
VAE为每个输入x，生成了一个潜在概率分布p(z|x)，然后再从分布中进行随机采样，从而得到了连续完整的潜在空间，解决了AE中无法用于生成的问题。
把原始输入x看作是一个表面特征，而其潜在特征便是表面经过抽象之后的类特征，它将比表面特征更具备区分事物的能力，而VAE直接拟合了基于已知的潜在概率分布，可以说是进一步的掌握了事物的本质。

为了避免计算 KL divergence 中的积分，我们使用重参数的技巧，不是每次产生一个隐含向量，而是生成两个向量，一个表示均值，一个表示标准差，这里我们默认编码之后的隐含向量服从一个正态分布的之后，就可以用一个标准正态分布先乘上标准差再加上均值来合成这个正态分布，最后 loss 就是希望这个生成的正态分布能够符合一个标准正态分布，也就是希望均值为 0，方差为 1 。


* 字体
*粗体*
/斜体/
+删除线+
_下划线_
下标： H_2 O
上标： E=mc^2
等宽字：  =git=  或者 ～git～
| Name  | Pone | Age |
|-------+------+-----|
| Peter | 1234 | 17  |
| Anna  | 4321 | 25  |

